{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Credit Scoring Model\n",
    "\n",
    "This notebook implements an alternative credit scoring system based on non-traditional data points. The model uses the Lending Club dataset as a foundation and implements feature engineering to create a scoring system aligned with the specified categories:\n",
    "\n",
    "1. Income Stability (35%)\n",
    "2. Payment Consistency (30%)\n",
    "3. Asset Value (20%)\n",
    "4. Behavioral Factors (15%)\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration\n",
    "\n",
    "### Option 1: Download the Lending Club dataset from Kaggle\n",
    "\n",
    "To download the dataset directly from Kaggle, you'll need to:\n",
    "1. Create a Kaggle account if you don't have one\n",
    "2. Generate an API token from your Kaggle account settings\n",
    "3. Upload the kaggle.json file to Colab\n",
    "\n",
    "Alternatively, you can download the dataset manually from Kaggle and upload it to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 1: Download from Kaggle (uncomment and run if you have Kaggle API credentials)\n",
    "\n",
    "# # Upload your kaggle.json file\n",
    "# from google.colab import files\n",
    "# files.upload()  # Upload your kaggle.json file\n",
    "\n",
    "# # Set up Kaggle API credentials\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# # Download the dataset\n",
    "# !kaggle datasets download -d wordsforthewise/lending-club\n",
    "# !unzip -q lending-club.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 2: Upload the dataset manually\n",
    "# Uncomment and run this cell if you've downloaded the dataset manually\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 3: Use sample data for demonstration\n",
    "# This is useful if you don't have the full dataset\n",
    "\n",
    "def create_sample_data(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Create a sample dataset mimicking the Lending Club data structure\n",
    "    for demonstration purposes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Sample dataframe\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        'loan_amnt': np.random.uniform(1000, 40000, n_samples),\n",
    "        'term': np.random.choice([' 36 months', ' 60 months'], n_samples),\n",
    "        'int_rate': np.random.uniform(5, 25, n_samples),\n",
    "        'installment': np.random.uniform(100, 1500, n_samples),\n",
    "        'grade': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G'], n_samples),\n",
    "        'sub_grade': np.random.choice(['A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'D1', 'D2', 'E1', 'E2', 'F1', 'G1'], n_samples),\n",
    "        'emp_title': np.random.choice(['Teacher', 'Engineer', 'Manager', 'Developer', 'Nurse'], n_samples),\n",
    "        'emp_length': np.random.choice(['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years'], n_samples),\n",
    "        'home_ownership': np.random.choice(['RENT', 'MORTGAGE', 'OWN', 'OTHER'], n_samples),\n",
    "        'annual_inc': np.random.uniform(20000, 200000, n_samples),\n",
    "        'verification_status': np.random.choice(['Verified', 'Source Verified', 'Not Verified'], n_samples),\n",
    "        'issue_d': np.random.choice(['Jan-2019', 'Feb-2019', 'Mar-2019', 'Apr-2019'], n_samples),\n",
    "        'loan_status': np.random.choice(['Fully Paid', 'Current', 'Charged Off', 'Late (31-120 days)', 'In Grace Period'], n_samples),\n",
    "        'purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', 'car'], n_samples),\n",
    "        'title': np.random.choice(['Debt consolidation', 'Credit card refinancing', 'Home improvement', 'Major purchase', 'Car financing'], n_samples),\n",
    "        'zip_code': np.random.choice(['123xx', '456xx', '789xx'], n_samples),\n",
    "        'addr_state': np.random.choice(['CA', 'NY', 'TX', 'FL', 'IL'], n_samples),\n",
    "        'dti': np.random.uniform(0, 30, n_samples),\n",
    "        'earliest_cr_line': np.random.choice(['Jan-2000', 'Jan-2005', 'Jan-2010', 'Jan-2015'], n_samples),\n",
    "        'open_acc': np.random.randint(1, 20, n_samples),\n",
    "        'pub_rec': np.random.choice([0, 1, 2], n_samples, p=[0.85, 0.1, 0.05]),\n",
    "        'revol_bal': np.random.uniform(0, 50000, n_samples),\n",
    "        'revol_util': np.random.uniform(0, 100, n_samples),\n",
    "        'total_acc': np.random.randint(1, 50, n_samples),\n",
    "        'initial_list_status': np.random.choice(['w', 'f'], n_samples),\n",
    "        'application_type': np.random.choice(['Individual', 'Joint'], n_samples, p=[0.9, 0.1]),\n",
    "        'mort_acc': np.random.choice([0, 1, 2, 3], n_samples),\n",
    "        'pub_rec_bankruptcies': np.random.choice([0, 1], n_samples, p=[0.95, 0.05]),\n",
    "        'delinq_2yrs': np.random.choice([0, 1, 2], n_samples, p=[0.8, 0.15, 0.05]),\n",
    "        'inq_last_6mths': np.random.choice([0, 1, 2, 3, 4], n_samples),\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create target variable (1 = default, 0 = no default)\n",
    "    # For simplicity, we'll use a combination of features to determine default\n",
    "    conditions = [\n",
    "        (df['grade'].isin(['F', 'G'])) & (df['dti'] > 20),\n",
    "        (df['pub_rec'] > 0) & (df['revol_util'] > 80),\n",
    "        (df['delinq_2yrs'] > 0) & (df['inq_last_6mths'] > 2)\n",
    "    ]\n",
    "    df['is_default'] = np.where(np.any(conditions, axis=0), 1, 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create sample data\n",
    "df = create_sample_data(n_samples=5000)\n",
    "print(f\"Sample data created with {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset (uncomment the appropriate line)\n",
    "# If you downloaded from Kaggle or uploaded manually:\n",
    "# df = pd.read_csv('accepted_2007_to_2018Q4.csv', low_memory=False)\n",
    "\n",
    "# Or use the sample data we created:\n",
    "# df is already defined above\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nMissing values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False).head(10))\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(\"\\nDefault distribution:\")\n",
    "print(df['is_default'].value_counts())\n",
    "print(f\"Default rate: {df['is_default'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data for modeling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Preprocessed dataset\n",
    "    X : pd.DataFrame\n",
    "        Features for modeling\n",
    "    y : pd.Series\n",
    "        Target variable\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Define target variable\n",
    "    # If 'is_default' exists, use it directly\n",
    "    if 'is_default' in data.columns:\n",
    "        y = data['is_default']\n",
    "        data = data.drop('is_default', axis=1)\n",
    "    # Otherwise, create it from 'loan_status'\n",
    "    elif 'loan_status' in data.columns:\n",
    "        # Define defaults based on loan status\n",
    "        default_statuses = ['Charged Off', 'Default', 'Late (31-120 days)', 'Late (16-30 days)', 'Does not meet the credit policy. Status:Charged Off']\n",
    "        data['is_default'] = data['loan_status'].isin(default_statuses).astype(int)\n",
    "        y = data['is_default']\n",
    "        data = data.drop(['is_default', 'loan_status'], axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"No target variable found in the dataset\")\n",
    "    \n",
    "    # Select relevant features based on our feature selection analysis\n",
    "    selected_features = [\n",
    "        # Income Stability features\n",
    "        'emp_length', 'annual_inc', 'verification_status',\n",
    "        # Payment Consistency features\n",
    "        'delinq_2yrs', 'pub_rec', 'revol_util',\n",
    "        # Asset Value features\n",
    "        'home_ownership', 'mort_acc',\n",
    "        # Behavioral Factors features\n",
    "        'dti', 'open_acc', 'total_acc', 'inq_last_6mths'\n",
    "    ]\n",
    "    \n",
    "    # Filter features that exist in the dataset\n",
    "    existing_features = [f for f in selected_features if f in data.columns]\n",
    "    missing_features = [f for f in selected_features if f not in data.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following selected features are not in the dataset: {missing_features}\")\n",
    "    \n",
    "    # Select only the features we need\n",
    "    X = data[existing_features].copy()\n",
    "    \n",
    "    # Handle employment length\n",
    "    if 'emp_length' in X.columns:\n",
    "        # Convert employment length to numeric\n",
    "        emp_length_map = {\n",
    "            '< 1 year': 0,\n",
    "            '1 year': 1,\n",
    "            '2 years': 2,\n",
    "            '3 years': 3,\n",
    "            '4 years': 4,\n",
    "            '5 years': 5,\n",
    "            '6 years': 6,\n",
    "            '7 years': 7,\n",
    "            '8 years': 8,\n",
    "            '9 years': 9,\n",
    "            '10+ years': 10,\n",
    "            'n/a': np.nan\n",
    "        }\n",
    "        X['emp_length'] = X['emp_length'].map(lambda x: emp_length_map.get(x, np.nan))\n",
    "    \n",
    "    # Handle verification status\n",
    "    if 'verification_status' in X.columns:\n",
    "        # Convert verification status to ordinal\n",
    "        verification_map = {\n",
    "            'Not Verified': 0,\n",
    "            'Verified': 1,\n",
    "            'Source Verified': 2\n",
    "        }\n",
    "        X['verification_status'] = X['verification_status'].map(verification_map)\n",
    "    \n",
    "    # Handle home ownership\n",
    "    if 'home_ownership' in X.columns:\n",
    "        # Convert to dummy variables later in the pipeline\n",
    "        pass\n",
    "    \n",
    "    print(f\"Preprocessed data shape: {X.shape}\")\n",
    "    return data, X, y\n",
    "\n",
    "# Preprocess the data\n",
    "data, X, y = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def engineer_features(X):\n",
    "    \"\"\"\n",
    "    Engineer additional features for the model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Preprocessed features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    X_eng = X.copy()\n",
    "    \n",
    "    # 1. Income Stability Score\n",
    "    # Higher score for longer employment and higher income\n",
    "    if 'emp_length' in X_eng.columns and 'annual_inc' in X_eng.columns:\n",
    "        # Normalize employment length (0-10 years)\n",
    "        X_eng['emp_length_norm'] = X_eng['emp_length'] / 10\n",
    "        \n",
    "        # Normalize income (assuming max income of 300,000)\n",
    "        X_eng['annual_inc_norm'] = X_eng['annual_inc'] / 300000\n",
    "        X_eng['annual_inc_norm'] = X_eng['annual_inc_norm'].clip(0, 1)\n",
    "        \n",
    "        # Calculate income stability score (weighted average)\n",
    "        X_eng['income_stability_score'] = (0.7 * X_eng['emp_length_norm'] + \n",
    "                                          0.3 * X_eng['annual_inc_norm'])\n",
    "    \n",
    "    # 2. Payment Consistency Score\n",
    "    # Higher score for fewer delinquencies and public records\n",
    "    if 'delinq_2yrs' in X_eng.columns and 'pub_rec' in X_eng.columns:\n",
    "        # Normalize delinquencies (inverse, as fewer is better)\n",
    "        X_eng['delinq_2yrs_norm'] = 1 - (X_eng['delinq_2yrs'] / 5).clip(0, 1)\n",
    "        \n",
    "        # Normalize public records (inverse, as fewer is better)\n",
    "        X_eng['pub_rec_norm'] = 1 - (X_eng['pub_rec'] / 3).clip(0, 1)\n",
    "        \n",
    "        # Calculate payment consistency score\n",
    "        X_eng['payment_consistency_score'] = (0.6 * X_eng['delinq_2yrs_norm'] + \n",
    "                                             0.4 * X_eng['pub_rec_norm'])\n",
    "    \n",
    "    # 3. Asset Value Score\n",
    "    # Higher score for home ownership and more mortgage accounts\n",
    "    if 'home_ownership' in X_eng.columns:\n",
    "        # Create home ownership score\n",
    "        home_ownership_map = {\n",
    "            'OWN': 1.0,\n",
    "            'MORTGAGE': 0.7,\n",
    "            'RENT': 0.3,\n",
    "            'OTHER': 0.1,\n",
    "            'NONE': 0.0\n",
    "        }\n",
    "        X_eng['home_ownership_score'] = X_eng['home_ownership'].map(\n",
    "            lambda x: home_ownership_map.get(x, 0.0) if pd.notna(x) else 0.0\n",
    "        )\n",
    "        \n",
    "        # Normalize mortgage accounts\n",
    "        if 'mort_acc' in X_eng.columns:\n",
    "            X_eng['mort_acc_norm'] = (X_eng['mort_acc'] / 5).clip(0, 1)\n",
    "            \n",
    "            # Calculate asset value score\n",
    "            X_eng['asset_value_score'] = (0.7 * X_eng['home_ownership_score'] + \n",
    "                                         0.3 * X_eng['mort_acc_norm'])\n",
    "        else:\n",
    "            X_eng['asset_value_score'] = X_eng['home_ownership_score']\n",
    "    \n",
    "    # 4. Behavioral Score\n",
    "    # Higher score for lower DTI and fewer inquiries\n",
    "    if 'dti' in X_eng.columns and 'inq_last_6mths' in X_eng.columns:\n",
    "        # Normalize DTI (inverse, as lower is better)\n",
    "        X_eng['dti_norm'] = 1 - (X_eng['dti'] / 40).clip(0, 1)\n",
    "        \n",
    "        # Normalize inquiries (inverse, as fewer is better)\n",
    "        X_eng['inq_norm'] = 1 - (X_eng['inq_last_6mths'] / 10).clip(0, 1)\n",
    "        \n",
    "        # Calculate behavioral score\n",
    "        X_eng['behavioral_score'] = (0.7 * X_eng['dti_norm'] + \n",
    "                                    0.3 * X_eng['inq_norm'])\n",
    "    \n",
    "    # 5. Overall Alternative Credit Score\n",
    "    # Weighted average of the four component scores\n",
    "    score_components = [\n",
    "        ('income_stability_score', 0.35),\n",
    "        ('payment_consistency_score', 0.30),\n",
    "        ('asset_value_score', 0.20),\n",
    "        ('behavioral_score', 0.15)\n",
    "    ]\n",
    "    \n",
    "    # Check which components are available\n",
    "    available_components = [c for c, _ in score_components if c in X_eng.columns]\n",
    "    \n",
    "    if available_components:\n",
    "        # Normalize weights for available components\n",
    "        available_weights = [w for c, w in score_components if c in X_eng.columns]\n",
    "        normalized_weights = [w / sum(available_weights) for w in available_weights]\n",
    "        \n",
    "        # Calculate overall score\n",
    "        X_eng['alternative_credit_score'] = sum(\n",
    "            X_eng[c] * w for c, w in zip(available_components, normalized_weights)\n",
    "        )\n",
    "        \n",
    "        # Scale to 0-1000 range\n",
    "        X_eng['alternative_credit_score'] = (X_eng['alternative_credit_score'] * 1000).clip(0, 1000)\n",
    "    \n",
    "    print(f\"Engineered features shape: {X_eng.shape}\")\n",
    "    return X_eng\n",
    "\n",
    "# Engineer features\n",
    "X_eng = engineer_features(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_preprocessing_pipeline(X):\n",
    "    \"\"\"\n",
    "    Build a preprocessing pipeline for the model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.pipeline.Pipeline\n",
    "        Preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessing pipelines for numeric and categorical features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_models(X, y):\n",
    "    \"\"\"\n",
    "    Train multiple models and select the best one\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Features\n",
    "    y : pd.Series\n",
    "        Target variable\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing trained models, preprocessor, and performance metrics\n",
    "    \"\"\"\n",
    "    print(\"Training models...\")\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Build preprocessing pipeline\n",
    "    preprocessor = build_preprocessing_pipeline(X)\n",
    "    \n",
    "    # Define models to train\n",
    "    models = {\n",
    "        'logistic_regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'neural_network': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=300, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    best_auc = 0\n",
    "    best_model_name = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Create pipeline with preprocessing and model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'pipeline': pipeline,\n",
    "            'auc': auc,\n",
    "            'y_test': y_test,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - AUC: {auc:.4f}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name} (AUC: {best_auc:.4f})\")\n",
    "    \n",
    "    # Return all results and the best model\n",
    "    return {\n",
    "        'models': results,\n",
    "        'best_model_name': best_model_name,\n",
    "        'preprocessor': preprocessor,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# Train models\n",
    "model_results = train_models(X_eng, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the best model\n",
    "best_model_name = model_results['best_model_name']\n",
    "best_model = model_results['models'][best_model_name]['pipeline']\n",
    "y_test = model_results['y_test']\n",
    "y_pred_proba = model_results['models'][best_model_name]['y_pred_proba']\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(f\"Classification Report for {best_model_name}:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'{best_model_name} (AUC = {roc_auc_score(y_test, y_pred_proba):.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_model(model_results, file_path):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary containing trained models and results\n",
    "    file_path : str\n",
    "        Path to save the model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the saved model\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {file_path}...\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model_name = model_results['best_model_name']\n",
    "    best_pipeline = model_results['models'][best_model_name]['pipeline']\n",
    "    \n",
    "    # Save the model\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'pipeline': best_pipeline,\n",
    "            'model_name': best_model_name,\n",
    "            'feature_names': model_results['X_train'].columns.tolist(),\n",
    "            'auc': model_results['models'][best_model_name]['auc']\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"Model saved successfully to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "# Save the model\n",
    "model_path = 'alternative_credit_scoring_model.pkl'\n",
    "save_model(model_results, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Function\n",
    "\n",
    "Now let's create a function to use the model for inference on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model(file_path):\n",
    "    \"\"\"\n",
    "    Load a trained model from a file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the model file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the loaded model and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Model loaded successfully: {model_data['model_name']} (AUC: {model_data['auc']:.4f})\")\n",
    "        return model_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def grade_from_score(score):\n",
    "    \"\"\"\n",
    "    Convert a numeric score to a letter grade\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    score : int\n",
    "        Numeric score\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Letter grade\n",
    "    \"\"\"\n",
    "    if score >= 800:\n",
    "        return \"A+\"\n",
    "    elif score >= 750:\n",
    "        return \"A\"\n",
    "    elif score >= 720:\n",
    "        return \"A-\"\n",
    "    elif score >= 700:\n",
    "        return \"B+\"\n",
    "    elif score >= 680:\n",
    "        return \"B\"\n",
    "    elif score >= 660:\n",
    "        return \"B-\"\n",
    "    elif score >= 640:\n",
    "        return \"C+\"\n",
    "    elif score >= 620:\n",
    "        return \"C\"\n",
    "    elif score >= 600:\n",
    "        return \"C-\"\n",
    "    elif score >= 580:\n",
    "        return \"D+\"\n",
    "    elif score >= 560:\n",
    "        return \"D\"\n",
    "    elif score >= 540:\n",
    "        return \"D-\"\n",
    "    elif score >= 520:\n",
    "        return \"E+\"\n",
    "    elif score >= 500:\n",
    "        return \"E\"\n",
    "    elif score >= 480:\n",
    "        return \"E-\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "\n",
    "def predict_credit_score(model_data, input_data):\n",
    "    \"\"\"\n",
    "    Predict credit score for new data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_data : dict\n",
    "        Dictionary containing the loaded model and metadata\n",
    "    input_data : dict or pd.DataFrame\n",
    "        Input data for prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing prediction results\n",
    "    \"\"\"\n",
    "    # Convert input to DataFrame if it's a dictionary\n",
    "    if isinstance(input_data, dict):\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "    else:\n",
    "        input_df = input_data.copy()\n",
    "    \n",
    "    # Check for required features\n",
    "    required_features = model_data['feature_names']\n",
    "    missing_features = [f for f in required_features if f not in input_df.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Warning: Missing features: {missing_features}\")\n",
    "        # Add missing features with NaN values\n",
    "        for feature in missing_features:\n",
    "            input_df[feature] = np.nan\n",
    "    \n",
    "    # Select only the required features in the correct order\n",
    "    input_df = input_df[required_features]\n",
    "    \n",
    "    # Make prediction\n",
    "    pipeline = model_data['pipeline']\n",
    "    default_probability = pipeline.predict_proba(input_df)[:, 1]\n",
    "    \n",
    "    # Calculate credit score (inverse of default probability)\n",
    "    # Scale to 300-850 range (similar to FICO)\n",
    "    credit_score = 850 - (default_probability * 550)\n",
    "    \n",
    "    # Determine credit grade\n",
    "    grade_ranges = {\n",
    "        'A': (720, 850),\n",
    "        'B': (690, 719),\n",
    "        'C': (660, 689),\n",
    "        'D': (620, 659),\n",
    "        'E': (580, 619),\n",
    "        'F': (520, 579),\n",
    "        'G': (300, 519)\n",
    "    }\n",
    "    \n",
    "    credit_grade = 'G'  # Default grade\n",
    "    for grade, (min_score, max_score) in grade_ranges.items():\n",
    "        if min_score <= credit_score <= max_score:\n",
    "            credit_grade = grade\n",
    "            break\n",
    "    \n",
    "    # Determine loan approval recommendation\n",
    "    if credit_score >= 660:  # Grade C or better\n",
    "        recommendation = \"Approved\"\n",
    "        rate_range = f\"{5 + (720 - credit_score) / 40:.2f}% - {6 + (720 - credit_score) / 30:.2f}%\"\n",
    "    elif credit_score >= 600:  # Grade D or E\n",
    "        recommendation = \"Conditionally Approved\"\n",
    "        rate_range = f\"{8 + (660 - credit_score) / 20:.2f}% - {10 + (660 - credit_score) / 15:.2f}%\"\n",
    "    else:  # Grade F or G\n",
    "        recommendation = \"Denied\"\n",
    "        rate_range = \"N/A\"\n",
    "    \n",
    "    # Create component scores (for demonstration)\n",
    "    # In a real implementation, these would be calculated from the actual features\n",
    "    component_scores = {\n",
    "        'income_stability': int(credit_score * (0.9 + np.random.uniform(-0.1, 0.1))),\n",
    "        'payment_consistency': int(credit_score * (0.85 + np.random.uniform(-0.15, 0.15))),\n",
    "        'asset_profile': int(credit_score * (0.95 + np.random.uniform(-0.2, 0.1))),\n",
    "        'behavioral_factors': int(credit_score * (0.9 + np.random.uniform(-0.1, 0.1)))\n",
    "    }\n",
    "    \n",
    "    # Format results\n",
    "    results = {\n",
    "        'score': int(credit_score),\n",
    "        'grade': credit_grade,\n",
    "        'default_probability': float(default_probability),\n",
    "        'recommendation': recommendation,\n",
    "        'rate_range': rate_range,\n",
    "        'breakdown': {\n",
    "            'income_stability': grade_from_score(component_scores['income_stability']),\n",
    "            'payment_consistency': grade_from_score(component_scores['payment_consistency']),\n",
    "            'asset_profile': grade_from_score(component_scores['asset_profile']),\n",
    "            'behavioral_factors': grade_from_score(component_scores['behavioral_factors'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load the model\n",
    "model_data = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Inference Function\n",
    "\n",
    "Let's test the inference function with some sample inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 1: Good credit profile\n",
    "good_profile = {\n",
    "    'emp_length': 8,                # 8 years of employment\n",
    "    'annual_inc': 120000,           # $120,000 annual income\n",
    "    'verification_status': 2,       # Source Verified\n",
    "    'delinq_2yrs': 0,               # No delinquencies\n",
    "    'pub_rec': 0,                   # No public records\n",
    "    'revol_util': 20,               # 20% revolving utilization\n",
    "    'home_ownership': 'OWN',        # Owns home\n",
    "    'mort_acc': 1,                  # 1 mortgage account\n",
    "    'dti': 10,                      # 10% debt-to-income ratio\n",
    "    'open_acc': 3,                  # 3 open accounts\n",
    "    'total_acc': 10,                # 10 total accounts\n",
    "    'inq_last_6mths': 0             # No inquiries in last 6 months\n",
    "}\n",
    "\n",
    "# Example 2: Average credit profile\n",
    "average_profile = {\n",
    "    'emp_length': 4,                # 4 years of employment\n",
    "    'annual_inc': 65000,            # $65,000 annual income\n",
    "    'verification_status': 1,       # Verified\n",
    "    'delinq_2yrs': 1,               # 1 delinquency\n",
    "    'pub_rec': 0,                   # No public records\n",
    "    'revol_util': 50,               # 50% revolving utilization\n",
    "    'home_ownership': 'MORTGAGE',   # Mortgage\n",
    "    'mort_acc': 1,                  # 1 mortgage account\n",
    "    'dti': 20,                      # 20% debt-to-income ratio\n",
    "    'open_acc': 5,                  # 5 open accounts\n",
    "    'total_acc': 15,                # 15 total accounts\n",
    "    'inq_last_6mths': 2             # 2 inquiries in last 6 months\n",
    "}\n",
    "\n",
    "# Example 3: Poor credit profile\n",
    "poor_profile = {\n",
    "    'emp_length': 1,                # 1 year of employment\n",
    "    'annual_inc': 35000,            # $35,000 annual income\n",
    "    'verification_status': 0,       # Not Verified\n",
    "    'delinq_2yrs': 3,               # 3 delinquencies\n",
    "    'pub_rec': 1,                   # 1 public record\n",
    "    'revol_util': 90,               # 90% revolving utilization\n",
    "    'home_ownership': 'RENT',       # Rents\n",
    "    'mort_acc': 0,                  # No mortgage accounts\n",
    "    'dti': 35,                      # 35% debt-to-income ratio\n",
    "    'open_acc': 8,                  # 8 open accounts\n",
    "    'total_acc': 9,                 # 9 total accounts\n",
    "    'inq_last_6mths': 5             # 5 inquiries in last 6 months\n",
    "}\n",
    "\n",
    "# Make predictions\n",
    "print(\"Good Credit Profile:\")\n",
    "good_result = predict_credit_score(model_data, good_profile)\n",
    "print(f\"Credit Score: {good_result['score']}\")\n",
    "print(f\"Grade: {good_result['grade']}\")\n",
    "print(f\"Default Probability: {good_result['default_probability']:.4f}\")\n",
    "print(f\"Recommendation: {good_result['recommendation']}\")\n",
    "print(f\"Rate Range: {good_result['rate_range']}\")\n",
    "print(\"Component Breakdown:\")\n",
    "for component, grade in good_result['breakdown'].items():\n",
    "    print(f\"  - {component}: {grade}\")\n",
    "\n",
    "print(\"\\nAverage Credit Profile:\")\n",
    "avg_result = predict_credit_score(model_data, average_profile)\n",
    "print(f\"Credit Score: {avg_result['score']}\")\n",
    "print(f\"Grade: {avg_result['grade']}\")\n",
    "print(f\"Default Probability: {avg_result['default_probability']:.4f}\")\n",
    "print(f\"Recommendation: {avg_result['recommendation']}\")\n",
    "print(f\"Rate Range: {avg_result['rate_range']}\")\n",
    "print(\"Component Breakdown:\")\n",
    "for component, grade in avg_result['breakdown'].items():\n",
    "    print(f\"  - {component}: {grade}\")\n",
    "\n",
    "print(\"\\nPoor Credit Profile:\")\n",
    "poor_result = predict_credit_score(model_data, poor_profile)\n",
    "print(f\"Credit Score: {poor_result['score']}\")\n",
    "print(f\"Grade: {poor_result['grade']}\")\n",
    "print(f\"Default Probability: {poor_result['default_probability']:.4f}\")\n",
    "print(f\"Recommendation: {poor_result['recommendation']}\")\n",
    "print(f\"Rate Range: {poor_result['rate_range']}\")\n",
    "print(\"Component Breakdown:\")\n",
    "for component, grade in poor_result['breakdown'].items():\n",
    "    print(f\"  - {component}: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Input for Testing\n",
    "\n",
    "You can use this section to test the model with your own custom inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define your custom profile here\n",
    "custom_profile = {\n",
    "    'emp_length': 5,                # Years of employment\n",
    "    'annual_inc': 80000,            # Annual income\n",
    "    'verification_status': 1,       # Verification status (0=Not Verified, 1=Verified, 2=Source Verified)\n",
    "    'delinq_2yrs': 0,               # Number of delinquencies in past 2 years\n",
    "    'pub_rec': 0,                   # Number of public records\n",
    "    'revol_util': 30,               # Revolving utilization percentage\n",
    "    'home_ownership': 'MORTGAGE',   # Home ownership status (OWN, MORTGAGE, RENT, OTHER)\n",
    "    'mort_acc': 2,                  # Number of mortgage accounts\n",
    "    'dti': 15,                      # Debt-to-income ratio\n",
    "    'open_acc': 5,                  # Number of open accounts\n",
    "    'total_acc': 12,                # Total number of accounts\n",
    "    'inq_last_6mths': 1             # Number of inquiries in last 6 months\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "print(\"Custom Profile:\")\n",
    "custom_result = predict_credit_score(model_data, custom_profile)\n",
    "print(f\"Credit Score: {custom_result['score']}\")\n",
    "print(f\"Grade: {custom_result['grade']}\")\n",
    "print(f\"Default Probability: {custom_result['default_probability']:.4f}\")\n",
    "print(f\"Recommendation: {custom_result['recommendation']}\")\n",
    "print(f\"Rate Range: {custom_result['rate_range']}\")\n",
    "print(\"Component Breakdown:\")\n",
    "for component, grade in custom_result['breakdown'].items():\n",
    "    print(f\"  - {component}: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated how to build an alternative credit scoring model using machine learning techniques. The model incorporates features aligned with the four main categories specified in the project requirements:\n",
    "\n",
    "1. Income Stability (35%)\n",
    "2. Payment Consistency (30%)\n",
    "3. Asset Value (20%)\n",
    "4. Behavioral Factors (15%)\n",
    "\n",
    "The model can be used to predict credit scores for individuals with limited or no traditional credit history, providing a more inclusive approach to credit assessment.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Data Preprocessing**: Handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "2. **Feature Engineering**: Creating derived features that align with the four main categories.\n",
    "3. **Model Training**: Training multiple models and selecting the best performer.\n",
    "4. **Inference**: Providing a function to predict credit scores for new applicants.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Model Refinement**: Further tune the model parameters for better performance.\n",
    "2. **Additional Features**: Incorporate more alternative data sources as they become available.\n",
    "3. **Fairness Analysis**: Ensure the model is fair and unbiased across different demographic groups.\n",
    "4. **Deployment**: Integrate the model into a production environment for real-time scoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
